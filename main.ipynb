{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","collapsed_sections":["BlIbbUfFY-5E","Qi-iaAQYAuux"],"machine_shape":"hm","authorship_tag":"ABX9TyPCtsAr1+pmIb+h7aGws0uO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dKjd-Kg-TgEZ","executionInfo":{"status":"ok","timestamp":1731965231510,"user_tz":-60,"elapsed":18089,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"203e03ad-b081-4769-add7-65b77a39498b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd drive/MyDrive/'Travail 3A'/INF554/Kaggle/challenge_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YQXZblkTzNw","executionInfo":{"status":"ok","timestamp":1731965232900,"user_tz":-60,"elapsed":1393,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"58bd1930-e43c-4202-ea8f-07c097978a17"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Travail 3A/INF554/Kaggle/challenge_data\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import gensim.downloader as api\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import torch\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"SLEy1XUIUA6O","executionInfo":{"status":"ok","timestamp":1731965240356,"user_tz":-60,"elapsed":7461,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Set the seed\n","set_seed(42)"],"metadata":{"id":"R3m6xzfRA16F","executionInfo":{"status":"ok","timestamp":1731965240356,"user_tz":-60,"elapsed":7,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Download some NLP models for processing, optional\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qb4m3zeuUQMA","executionInfo":{"status":"ok","timestamp":1731965240356,"user_tz":-60,"elapsed":6,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"22cd0687-8c35-4772-83ef-e1648d578659"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Load GloVe model with Gensim's API # 7min\n","embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"],"metadata":{"id":"ls-z5dvXUU5I","executionInfo":{"status":"ok","timestamp":1731965515673,"user_tz":-60,"elapsed":275322,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"083e16c2-7b54-40d6-972e-17e3abbd0799"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 758.5/758.5MB downloaded\n"]}]},{"cell_type":"code","source":["# Basic preprocessing function\n","def preprocess_text(text):\n","    # Lowercasing\n","    text = text.lower()\n","    # Remove punctuation\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","    # Tokenization\n","    words = text.split()\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words]\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    return ' '.join(words)"],"metadata":{"id":"CYZcM6TFYgh4","executionInfo":{"status":"ok","timestamp":1731965515675,"user_tz":-60,"elapsed":8,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Read all training files and concatenate them into one dataframe\n","li = []\n","for filename in os.listdir(\"train_tweets\"):\n","    df = pd.read_csv(\"train_tweets/\" + filename)\n","    li.append(df)\n","df = pd.concat(li, ignore_index=True)"],"metadata":{"id":"IjYsBb-lYlS5","executionInfo":{"status":"ok","timestamp":1731965538919,"user_tz":-60,"elapsed":23250,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# NE PAS RUN"],"metadata":{"id":"fzkmYwirDYWB"}},{"cell_type":"code","source":["# Apply preprocessing to each tweet # 13 min\n","df['Tweet'] = df['Tweet'].apply(preprocess_text)"],"metadata":{"id":"xSW9M6XB4c75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to compute the average word vector for a tweet\n","def get_avg_embedding(tweet, model, vector_size=200):\n","    words = tweet.split()  # Tokenize by whitespace\n","    word_vectors = [model[word] for word in words if word in model]\n","    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n","        return np.zeros(vector_size)\n","    return np.mean(word_vectors, axis=0)"],"metadata":{"id":"g004LUdxYStv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply preprocessing to each tweet and obtain vectors\n","vector_size = 200  # Adjust based on the chosen GloVe model\n","tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df['Tweet']])\n","tweet_df = pd.DataFrame(tweet_vectors)"],"metadata":{"id":"ZM6HOScsYqyT","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1731410109446,"user_tz":-60,"elapsed":364,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"1a469dad-915c-4d8d-aa4d-dcf366340ad0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'np' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7c198048b420>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply preprocessing to each tweet and obtain vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m  \u001b[0;31m# Adjust based on the chosen GloVe model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_avg_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtweet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","source":["# Attach the vectors into the original dataframe\n","period_features = pd.concat([df, tweet_df], axis=1)\n","# Drop the columns that are not useful anymore\n","period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n","# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n","period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n","\n","# We drop the non-numerical features and keep the embeddings values for each period\n","X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n","# We extract the labels of our training samples\n","y = period_features['EventType'].values"],"metadata":{"id":"f6TVrW7iYvcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###### Evaluating on a test set:\n","\n","# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n","# validation set and without submitting too many times into Kaggle\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# We set up a basic classifier that we train and then calculate the accuracy on our test set\n","clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print(\"Test set: \", accuracy_score(y_test, y_pred))"],"metadata":{"id":"L673PXuzYzJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# KAGGLE SUB"],"metadata":{"id":"BlIbbUfFY-5E"}},{"cell_type":"code","source":["# This time we train our classifier on the full dataset that it is available to us.\n","clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n","# We add a dummy classifier for sanity purposes\n","dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n","\n","predictions = []\n","dummy_predictions = []"],"metadata":{"id":"hAnnP6i5Y2uG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n","# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n","# to be submitted on Kaggle.\n","for fname in os.listdir(\"eval_tweets\"):\n","    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n","    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n","\n","    tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in val_df['Tweet']])\n","    tweet_df = pd.DataFrame(tweet_vectors)\n","\n","    period_features = pd.concat([val_df, tweet_df], axis=1)\n","    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n","    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n","    X = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n","\n","    preds = clf.predict(X)\n","    dummy_preds = dummy_clf.predict(X)\n","\n","    period_features['EventType'] = preds\n","    period_features['DummyEventType'] = dummy_preds\n","\n","    predictions.append(period_features[['ID', 'EventType']])\n","    dummy_predictions.append(period_features[['ID', 'DummyEventType']])"],"metadata":{"id":"rChE15cTZAz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_df = pd.concat(predictions)\n","pred_df.to_csv('logistic_predictions.csv', index=False)\n","\n","pred_df = pd.concat(dummy_predictions)\n","pred_df.to_csv('dummy_predictions.csv', index=False)"],"metadata":{"id":"r2CDidz7ZFT6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSTM tweet wise + Voting system"],"metadata":{"id":"Qi-iaAQYAuux"}},{"cell_type":"code","source":["print(df.shape)\n","print(df.columns)\n","\n","sub_size = 1000\n","selected_indexes = np.random.choice(df.index, size=sub_size, replace=False)\n","sub_df = df.loc[selected_indexes]\n","\n","print(sub_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbgYO_jxAwWe","executionInfo":{"status":"ok","timestamp":1731411584969,"user_tz":-60,"elapsed":497,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"aa3ee85e-3413-4be6-d044-addb3e2c7f59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(5056050, 6)\n","Index(['ID', 'MatchID', 'PeriodID', 'EventType', 'Timestamp', 'Tweet'], dtype='object')\n","(1000, 6)\n"]}]},{"cell_type":"code","source":["# droper tout les attributs à part la colonne tweet et la colonne Event_type\n","sub_y = sub_df['EventType']\n","sub_X = sub_df.drop(columns=['EventType', 'ID' ,'MatchID', 'PeriodID', 'Timestamp'])"],"metadata":{"id":"An_vT42XImrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_embedding(tweet, model=embeddings_model, vector_size=200):\n","    words = tweet.split()  # Tokenize by whitespace\n","    word_vectors = [model[word] for word in words if word in model]\n","    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n","        word_vectors = [np.zeros(vector_size)]\n","    return word_vectors"],"metadata":{"id":"_1uuR65hJXUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet_embeddings = sub_X['Tweet'].apply(get_embedding).to_numpy()"],"metadata":{"id":"ikaw5BB0Le4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print(tweet_embeddings.shape)\n","#print(tweet_embeddings.dtype)\n","#print(type(tweet_embeddings[0]))\n","#print(len(tweet_embeddings[0]))\n","\n","for x in range(10):\n","  print(len(tweet_embeddings[x]))\n","  print(sub_X[x:x + 1]['Tweet'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4D6SoEI0LsKV","executionInfo":{"status":"ok","timestamp":1731412143287,"user_tz":-60,"elapsed":304,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"02b1a74f-3832-477f-aa78-99a92085c975"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","434409    rt mictwizzle argentina better crush germany\n","Name: Tweet, dtype: object\n","4\n","736785    niallsuitandtie germany bc holland lost argent...\n","Name: Tweet, dtype: object\n","4\n","3843594    rt iquotecomedy brazil defender like httptcoua...\n","Name: Tweet, dtype: object\n","5\n","2632297    rt zjahr jeez germany playing terrible\n","Name: Tweet, dtype: object\n","9\n","830738    rt fifaworldcup social see whats said worldcup...\n","Name: Tweet, dtype: object\n","6\n","1845505    goooooal fra pogba nod french front\n","Name: Tweet, dtype: object\n","10\n","650416    jesus germany shape defense please like enough...\n","Name: Tweet, dtype: object\n","11\n","3183820    already know germany got one ill watching port...\n","Name: Tweet, dtype: object\n","7\n","2215962    rt fifaworldcup photo matshummels effort goal ...\n","Name: Tweet, dtype: object\n","13\n","895933    rt khatuberry argentina coach super hyper migh...\n","Name: Tweet, dtype: object\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import numpy as np\n","from tqdm import tqdm\n","\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels, embedding_model):\n","        self.texts = texts\n","        self.labels = labels\n","        self.embedding_model = embedding_model\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        # Get embeddings for the text\n","        embeddings = torch.tensor(self.embedding_model.encode(text), dtype=torch.float32)\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return embeddings, label\n","\n","def collate_fn(batch):\n","    # Separate embeddings and labels\n","    embeddings, labels = zip(*batch)\n","\n","    # Pad sequences to max length in batch\n","    padded_embeddings = pad_sequence(embeddings, batch_first=True)\n","    labels = torch.stack(labels)\n","\n","    return padded_embeddings, labels\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, num_classes, num_layers=1, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=embedding_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, embedding_dim)\n","        lstm_out, (hidden, _) = self.lstm(x)\n","        # Use last hidden state\n","        out = self.dropout(hidden[-1])\n","        return self.fc(out)\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        train_loss = 0\n","        train_correct = 0\n","        train_total = 0\n","\n","        for embeddings, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(embeddings)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            train_total += labels.size(0)\n","            train_correct += predicted.eq(labels).sum().item()\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        val_correct = 0\n","        val_total = 0\n","\n","        with torch.no_grad():\n","            for embeddings, labels in val_loader:\n","                embeddings, labels = embeddings.to(device), labels.to(device)\n","                outputs = model(embeddings)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                val_total += labels.size(0)\n","                val_correct += predicted.eq(labels).sum().item()\n","\n","        # Print metrics\n","        print(f'Epoch {epoch+1}:')\n","        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%')\n","        print(f'Val Loss: {val_loss/len(val_loader):.4f}, Acc: {100.*val_correct/val_total:.2f}%')\n","\n","        # Save best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), 'best_model.pt')\n","\n","# Example usage:\n","\"\"\"\n","# Assuming you have:\n","# - df: DataFrame with 'text' and 'label' columns\n","# - embedding_model: your embedding model\n","# - train_df, val_df: train/val splits of your data\n","\n","# Initialize dataset and dataloader\n","train_dataset = TextDataset(train_df['text'].values, train_df['label'].values, embedding_model)\n","val_dataset = TextDataset(val_df['text'].values, val_df['label'].values, embedding_model)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=32,\n","    shuffle=True,\n","    collate_fn=collate_fn\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=32,\n","    shuffle=False,\n","    collate_fn=collate_fn\n",")\n","\n","# Initialize model and training components\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = LSTMClassifier(\n","    embedding_dim=768,  # Adjust based on your embedding model\n","    hidden_dim=128,\n","    num_classes=num_classes,  # Number of classes in your task\n","    num_layers=2,\n","    dropout=0.1\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train the model\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device)\n","\"\"\""],"metadata":{"id":"Ce0OfZ_KMWz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# START"],"metadata":{"id":"MqhsSKHH7MXe"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import numpy as np\n","from tqdm import tqdm\n","from torch import optim\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"rV9Lj4Xm9SEX","executionInfo":{"status":"ok","timestamp":1731965557194,"user_tz":-60,"elapsed":309,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_embedding(tweet, model=embeddings_model, vector_size=200):\n","    words = tweet.split()  # Tokenize by whitespace\n","    word_vectors = [model[word] for word in words if word in model]\n","    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n","        word_vectors = [np.zeros(vector_size)]\n","    return word_vectors"],"metadata":{"id":"2Mxqmt-E-dI4","executionInfo":{"status":"ok","timestamp":1731965557404,"user_tz":-60,"elapsed":2,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(df.shape)\n","print(df.columns)\n","print(df['EventType'].sum() / df.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcONMalU7NnZ","executionInfo":{"status":"ok","timestamp":1731965557932,"user_tz":-60,"elapsed":2,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"3cf04bb4-f8b2-49e2-bc23-0a2f797942ae"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(5056050, 6)\n","Index(['ID', 'MatchID', 'PeriodID', 'EventType', 'Timestamp', 'Tweet'], dtype='object')\n","0.5646836957704137\n"]}]},{"cell_type":"code","source":["# pour l'instant\n","df = df.drop(columns=['ID', 'MatchID', 'PeriodID', 'Timestamp'])\n","y = df['EventType']\n","X = df.drop(columns=['EventType'])"],"metadata":{"id":"kp6WgTQg7Pk6","executionInfo":{"status":"ok","timestamp":1731965558522,"user_tz":-60,"elapsed":318,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95, random_state=42)\n","X_val, X_test_2, y_val, y_test_2 = train_test_split(X_test, y_test, test_size=0.99, random_state=42)"],"metadata":{"id":"he6JARKs78Pi","executionInfo":{"status":"ok","timestamp":1731969803203,"user_tz":-60,"elapsed":3121,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["print(X_val.shape)\n","print(X_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcGjU5GA-jWD","executionInfo":{"status":"ok","timestamp":1731969803203,"user_tz":-60,"elapsed":3,"user":{"displayName":"User 210","userId":"08803030925180574152"}},"outputId":"4dc7d156-8cc4-4f64-af67-ac66609fd898"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["(48032, 1)\n","(252802, 1)\n"]}]},{"cell_type":"code","source":["class LSTMClassifier(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n","        super(LSTMClassifier, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=False,  # Changed to False for better memory efficiency\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x, apply_softmax=False):\n","        # x shape: (seq_length, batch_size, input_size)\n","        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(x.device)\n","\n","        out, _ = self.lstm(x, (h0, c0))\n","        # Use last time step output\n","        out = out[-1]  # Changed from out[:, -1, :] to out[-1]\n","        out = self.fc(out)\n","\n","        if apply_softmax:\n","            out = self.softmax(out)\n","        return out"],"metadata":{"id":"j4o3pwg38eap","executionInfo":{"status":"ok","timestamp":1731969803204,"user_tz":-60,"elapsed":3,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["model = LSTMClassifier(\n","        input_size=200,\n","        hidden_size=64,\n","        num_layers=3,\n","        num_classes=2,\n","        dropout=0.3\n","    ).to('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"hyHNFe7V9O96","executionInfo":{"status":"ok","timestamp":1731969803204,"user_tz":-60,"elapsed":3,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["epochs = 50\n","batch_size = 1024 * 2 * 2\n","lr = 0.006"],"metadata":{"id":"DGqKjLX-9hMa","executionInfo":{"status":"ok","timestamp":1731969803204,"user_tz":-60,"elapsed":3,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["def preprocess_df(init_df, indices=None):\n","  if(indices is None) : indices = torch.arange(len(init_df))\n","\n","  batch = init_df.iloc[indices].copy()\n","\n","  batch['Tweet'] = batch['Tweet'].apply(preprocess_text)\n","  batch['Tweet'] = batch['Tweet'].apply(get_embedding)\n","\n","  max_len = max(len(seq) for seq in batch['Tweet'].values)\n","\n","  batch['Tweet'] = batch['Tweet'].apply(lambda seq: np.array((seq + [np.zeros(200)] * (max_len - len(seq))), dtype=np.float32))\n","\n","  HHB = np.stack(batch['Tweet'].values)\n","\n","  batch = torch.tensor(HHB).to(device)\n","  batch = batch.transpose(0, 1)\n","\n","  return batch"],"metadata":{"id":"ZsWT4V_iBP5k","executionInfo":{"status":"ok","timestamp":1731969803204,"user_tz":-60,"elapsed":3,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["def eval_perf(df, y_gt, label='val'):\n","  model.eval()\n","\n","  y_gt = torch.tensor(y_gt.copy().values).to(device)\n","\n","  criterion = nn.CrossEntropyLoss()\n","\n","  input = preprocess_df(df)\n","  y_pred = model(input)\n","\n","  print('Loss ' + label + ' : ', criterion(y_pred, y_gt).item())\n","  # Move y_pred to CPU before converting to NumPy array\n","  print('Accuracy ' + label + ' : ', accuracy_score(y_gt.cpu(), y_pred.cpu().argmax(dim=1)), '\\n')\n","  model.train()"],"metadata":{"id":"rgHuNwB_Ptvp","executionInfo":{"status":"ok","timestamp":1731969805356,"user_tz":-60,"elapsed":198,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["def train_LSTM(model, X_train, y_train, X_test, y_test, epochs, batch_size, lr, freq_train=1, freq_val=1):\n","  # on définit le dataloader sur les indexes\n","  train_loader = DataLoader(torch.arange(len(X_train)), batch_size=batch_size, shuffle=True)\n","\n","  optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","  criterion = nn.CrossEntropyLoss()\n","\n","  for i in range(epochs):\n","\n","    epoch_loss = 0.\n","\n","    for (j, batch_indices) in enumerate(train_loader):\n","\n","      batch_labels = y_train.iloc[batch_indices].copy()\n","      batch_labels = torch.tensor(batch_labels.values).to(device)\n","\n","\n","      # on process les données :\n","      batch = X_train.iloc[batch_indices].copy()\n","      batch['Tweet'] = batch['Tweet'].apply(preprocess_text)\n","      batch['Tweet'] = batch['Tweet'].apply(get_embedding)\n","\n","      #if(i == 0 and j == 0) :\n","\n","      max_len = max(len(seq) for seq in batch['Tweet'].values)\n","\n","      # Pad sequences with torch.zeros(200)\n","      batch['Tweet'] = batch['Tweet'].apply(lambda seq: np.array((seq + [np.zeros(200)] * (max_len - len(seq))), dtype=np.float32))\n","\n","      HHB = np.stack(batch['Tweet'].values)\n","\n","      batch = torch.tensor(HHB).to(device)\n","      batch = batch.transpose(0, 1)\n","\n","      y_pred = model(batch)\n","\n","      loss = criterion(y_pred, batch_labels)\n","\n","      epoch_loss += loss.item()\n","\n","      #if((i + j) % freq_print == 0) : print('Loss epoch ', i, ' : ', loss.item(), '\\n')\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","    if((i % freq_val == 0) or (i % freq_train == 0)):\n","      print('Epoch : ', i , '\\n')\n","\n","      if(i % freq_train == 0) : print('Loss train : ', epoch_loss / len(train_loader), '\\n')\n","      if(i % freq_val == 0) : eval_perf(X_val, y_val)"],"metadata":{"id":"oc0J6mpf9pAL","executionInfo":{"status":"ok","timestamp":1731969811218,"user_tz":-60,"elapsed":216,"user":{"displayName":"User 210","userId":"08803030925180574152"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["train_LSTM(model, X_train, y_train, X_test, y_test, epochs, batch_size, lr, freq_train=1, freq_val=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rr91aVApBb4q","outputId":"674d41c6-f31d-43ec-8b90-4161b1422aec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch :  0 \n","\n","Loss train :  0.6760094348461397 \n","\n","Loss val :  0.6590946912765503\n","Accuracy val :  0.5876915389740173 \n","\n","Epoch :  1 \n","\n","Loss train :  0.6520076980513911 \n","\n","Epoch :  2 \n","\n","Loss train :  0.6395111295484728 \n","\n","Epoch :  3 \n","\n","Loss train :  0.6302119753053111 \n","\n","Epoch :  4 \n","\n","Loss train :  0.6217824326407525 \n","\n","Epoch :  5 \n","\n","Loss train :  0.6141334137608928 \n","\n","Epoch :  6 \n","\n","Loss train :  0.6066555284684704 \n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"suWKO21tb-f2"},"execution_count":null,"outputs":[]}]}