{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/amine.chraibi/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/amine.chraibi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/amine.chraibi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import optuna\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "max_length = 44\n",
    "MAX_TWEETS= 650\n",
    "PAD_TWEET = \"[PAD]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters \n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "# count number of words \n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# count number of capital characters\n",
    "def count_capital_chars(text):\n",
    "    count=0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "# count number of capital words\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "\n",
    "# count number of punctuations\n",
    "def count_punctuations(text):\n",
    "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d\n",
    "\n",
    "# count number of words in quotes\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(\"\\'.\\'|\\\".\\\"\", text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count\n",
    "    \n",
    "# count number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "# calculate average word length\n",
    "def avg_word_len(char_cnt,word_cnt):\n",
    "    return char_cnt/word_cnt\n",
    "\n",
    "# calculate average sentence length\n",
    "def avg_sent_len(word_cnt,sent_cnt):\n",
    "    return word_cnt/sent_cnt\n",
    "\n",
    "# count number of unique words \n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "            \n",
    "# words vs unique feature\n",
    "def words_vs_unique(words,unique):\n",
    "    return unique/words\n",
    "\n",
    "# count of hashtags\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(\\#\\w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "# count of mentions\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(\\@\\w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "# count of stopwords\n",
    "def count_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n",
    "# stopwords vs words\n",
    "def stopwords_vs_words(stopwords_cnt,text):\n",
    "    return stopwords_cnt/len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df[\"Tweet\"].apply(lambda x:count_chars(x))\n",
    "df['word_count'] = df[\"Tweet\"].apply(lambda x:count_words(x))\n",
    "df['sent_count'] = df[\"Tweet\"].apply(lambda x:count_sent(x))\n",
    "df['capital_char_count'] = df[\"Tweet\"].apply(lambda x:count_capital_chars(x))\n",
    "df['capital_word_count'] = df[\"Tweet\"].apply(lambda x:count_capital_words(x))\n",
    "df['quoted_word_count'] = df[\"Tweet\"].apply(lambda x:count_words_in_quotes(x))\n",
    "df['stopword_count'] = df[\"Tweet\"].apply(lambda x:count_stopwords(x))\n",
    "df['unique_word_count'] = df[\"Tweet\"].apply(lambda x:count_unique_words(x))\n",
    "df['htag_count'] = df[\"Tweet\"].apply(lambda x:count_htags(x))\n",
    "df['mention_count'] = df[\"Tweet\"].apply(lambda x:count_mentions(x))\n",
    "df['punct_count'] = df[\"Tweet\"].apply(lambda x:count_punctuations(x))\n",
    "df['avg_wordlength']=df['char_count']/df['word_count']\n",
    "df['avg_sentlength']=df['word_count']/df['sent_count']\n",
    "df['unique_vs_words']=df['unique_word_count']/df['word_count']\n",
    "df['stopwords_vs_words']=df['stopword_count']/df['word_count']\n",
    "df['Tweet'] = df['Tweet'].apply(utils.preprocess_text)\n",
    "df['Tweet'] = df['Tweet'].astype('string')\n",
    "df['Tweet'].fillna('', inplace = True)\n",
    "df_punct = pd.DataFrame(list(df.punct_count))\n",
    "\n",
    "df=pd.merge(df,df_punct,left_index=True, right_index=True)\n",
    "\n",
    "df.drop(columns=['punct_count'],inplace=True)\n",
    "\n",
    "with open(\"preprocessed_data2.pkl\", 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('preprocessed_data2.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =  TfidfVectorizer(max_features=500)\n",
    "tf_idf_features =  vectorizer.fit_transform(df['Tweet'])\n",
    "tf_idf          = pd.DataFrame(tf_idf_features)\n",
    "stacked_df = pd.concat([df.reset_index(drop=True), tf_idf.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = ['MatchID', 'PeriodID']\n",
    "stacked_df = stacked_df.drop(columns=['Timestamp'])\n",
    "stacked_df['TweetCount'] = stacked_df.groupby(group_cols)['Tweet'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Optimize both train and test DataFrames\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m stacked_df \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36moptimize_dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     17\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[col], downcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m col_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     num_unique \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     num_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[col])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Convert to 'category' if the number of unique values is less than 50% of total entries\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/base.py:1063\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[0;34m(self, dropna)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     uniqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropna:\n\u001b[1;32m   1065\u001b[0m         uniqs \u001b[38;5;241m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7248\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## For efficient memory management\n",
    "## This cell looks like it doesn't work as it prints an error but it has the intended effect (you can check stacked_df.info() before\n",
    "# and after this cell)\n",
    "def optimize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Optimize the data types of a DataFrame to reduce memory usage.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'int64':\n",
    "            if df[col].min() >= 0:\n",
    "                df[col] = pd.to_numeric(df[col], downcast='unsigned')\n",
    "            else:\n",
    "                df[col] = pd.to_numeric(df[col], downcast='signed')\n",
    "        \n",
    "        elif col_type == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        \n",
    "        elif col_type == 'object':\n",
    "            num_unique = df[col].nunique()\n",
    "            num_total = len(df[col])\n",
    "            if num_unique / num_total < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "stacked_df = optimize_dataframe(stacked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5056050 entries, 0 to 5056049\n",
      "Data columns (total 53 columns):\n",
      " #   Column              Dtype   \n",
      "---  ------              -----   \n",
      " 0   ID                  category\n",
      " 1   MatchID             uint8   \n",
      " 2   PeriodID            uint8   \n",
      " 3   EventType           uint8   \n",
      " 4   Tweet               string  \n",
      " 5   char_count          uint8   \n",
      " 6   word_count          uint8   \n",
      " 7   sent_count          uint8   \n",
      " 8   capital_char_count  uint8   \n",
      " 9   capital_word_count  uint8   \n",
      " 10  quoted_word_count   uint8   \n",
      " 11  stopword_count      uint8   \n",
      " 12  unique_word_count   uint8   \n",
      " 13  htag_count          uint8   \n",
      " 14  mention_count       uint8   \n",
      " 15  avg_wordlength      float32 \n",
      " 16  avg_sentlength      float32 \n",
      " 17  unique_vs_words     float32 \n",
      " 18  stopwords_vs_words  float32 \n",
      " 19  ! count             uint8   \n",
      " 20  \" count             uint8   \n",
      " 21  # count             uint8   \n",
      " 22  $ count             uint8   \n",
      " 23  % count             uint8   \n",
      " 24  & count             uint8   \n",
      " 25  ' count             uint8   \n",
      " 26  ( count             uint8   \n",
      " 27  ) count             uint8   \n",
      " 28  * count             uint8   \n",
      " 29  + count             uint8   \n",
      " 30  , count             uint8   \n",
      " 31  - count             uint8   \n",
      " 32  . count             uint8   \n",
      " 33  / count             uint8   \n",
      " 34  : count             uint8   \n",
      " 35  ; count             uint8   \n",
      " 36  < count             uint8   \n",
      " 37  = count             uint8   \n",
      " 38  > count             uint8   \n",
      " 39  ? count             uint8   \n",
      " 40  @ count             uint8   \n",
      " 41  [ count             uint8   \n",
      " 42  \\ count             uint8   \n",
      " 43  ] count             uint8   \n",
      " 44  ^ count             uint8   \n",
      " 45  _ count             uint8   \n",
      " 46  ` count             uint8   \n",
      " 47  { count             uint8   \n",
      " 48  | count             uint8   \n",
      " 49  } count             uint8   \n",
      " 50  ~ count             uint8   \n",
      " 51  0                   object  \n",
      " 52  TweetCount          int64   \n",
      "dtypes: category(1), float32(4), int64(1), object(1), string(1), uint8(45)\n",
      "memory usage: 419.6+ MB\n"
     ]
    }
   ],
   "source": [
    "stacked_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = ['MatchID', 'PeriodID']\n",
    "agg_dict = {\n",
    "    col: 'first' if col == 'EventType' else 'count'\n",
    "    for col in stacked_df.columns\n",
    "    if col not in group_cols\n",
    "}\n",
    "\n",
    "agg_dict['Tweet'] = lambda x: list(x)\n",
    "agg_dict['TweetCount'] = 'first'\n",
    "\n",
    "agg_df = stacked_df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "grouped_labels = stacked_df.groupby(['MatchID', 'PeriodID'])['EventType'].max().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def prepare_features_and_labels(grouped_tweets, grouped_labels, bert_model, tokenizer, output_file=\"features_labels.pkl\"):\n",
    "    \"\"\"\n",
    "The function `prepare_features_and_labels` is preparing features and labels using BERT embeddings and saving them to a file. \n",
    "It takes as input parameters a DataFrame `grouped_tweets` with columns MatchID, PeriodID, and OriginalTweets, \n",
    "another DataFrame `grouped_labels` with labels (EventType) by MatchID and PeriodID, a pre-trained BERT model `bert_model`,\n",
    "a BERT tokenizer `tokenizer`, and an optional output file name `output_file` (default is \"features_labels.pkl\").\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Chargement des features et labels depuis {output_file}...\")\n",
    "        with open(output_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data[\"grouped_tweets\"], data[\"labels\"]\n",
    "\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "\n",
    "    print(\"Calcul des embeddings et labels...\")\n",
    "    for idx, row in tqdm(grouped_tweets.iterrows()):\n",
    "        tweets = row['Tweet']\n",
    "        if not isinstance(tweets, list) or len(tweets) == 0:\n",
    "            embeddings.append(torch.zeros(bert_model.config.hidden_size))  \n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            tweets,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            embed = bert_model.embeddings.word_embeddings(tokenized['input_ids'])\n",
    "        \n",
    "        features_mean = embed.mean(dim=1).mean(dim=0)  # Average across tokens, then across tweets\n",
    "        embeddings.append(features_mean)\n",
    "\n",
    "        match_id, period_id = row['MatchID'], row['PeriodID']\n",
    "        labels.append(grouped_labels.loc[match_id, period_id])\n",
    "\n",
    "    grouped_tweets['Embedding'] = embeddings\n",
    "\n",
    "    print(f\"Sauvegarde des features et labels dans {output_file}...\")\n",
    "    grouped_tweets = grouped_tweets.drop(columns=['Tweet'])\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump({\"grouped_tweets\": grouped_tweets, \"labels\": labels}, f)\n",
    "    return grouped_tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul des embeddings et labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2137it [1:06:35,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde des features et labels dans features_labels_lgbm_spacy_new_features.pkl...\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "features, labels = prepare_features_and_labels(\n",
    "    grouped_tweets=agg_df,\n",
    "    grouped_labels=grouped_labels,\n",
    "    bert_model=bert_model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_file=\"features_labels_lgbm_spacy_new_features.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LightGBM model...\n",
      "Trained model saved as lightgbm_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# =========================\n",
    "# Step 1: Extract Embeddings\n",
    "# =========================\n",
    "\n",
    "# Function to convert torch tensors to numpy arrays\n",
    "def convert_embedding(embedding_tensor):\n",
    "    return embedding_tensor.detach().cpu().numpy()\n",
    "\n",
    "# Extract and convert embeddings for training set\n",
    "embedding = np.vstack(features['Embedding'].apply(convert_embedding).values)\n",
    "\n",
    "columns_to_drop = ['EventType', 'MatchID', 'PeriodID', 'Embedding'] # We drop the embeddings here because we hstack with embedding later\n",
    "\n",
    "X_features = features.drop(columns=columns_to_drop).values\n",
    "\n",
    "\n",
    "# Which is why we dropped 'Embedding' earlier\n",
    "X = np.hstack([X_features, embedding])\n",
    "y = features['EventType'].values\n",
    "\n",
    "# Best parameters obtained with Optuna\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.020559564255188685,\n",
    "    'num_leaves': 175,\n",
    "    'max_depth': 38,\n",
    "    'min_data_in_leaf': 24,\n",
    "    'feature_fraction': 0.8206274379058552,\n",
    "    'bagging_fraction': 0.5008300119237329,\n",
    "    'bagging_freq': 9,\n",
    "    'lambda_l1': 3.0495576636476773,\n",
    "    'lambda_l2': 0.05257346531813992,\n",
    "    'min_gain_to_split': 0.26065506527165566,\n",
    "    'n_estimators': 759,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM classifier\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "# Train the model on the training data\n",
    "print(\"Training the LightGBM model...\")\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "# Save the trained model for future use\n",
    "model_filename = 'lightgbm_model.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "print(f\"Trained model saved as {model_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"eval_tweets\"):\n",
    "    df = pd.read_csv(\"eval_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df_eval = pd.concat(li, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805602/2669930065.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_eval['Tweet'].fillna('', inplace = True)\n"
     ]
    }
   ],
   "source": [
    "df_eval['char_count'] = df_eval[\"Tweet\"].apply(lambda x:count_chars(x))\n",
    "df_eval['word_count'] = df_eval[\"Tweet\"].apply(lambda x:count_words(x))\n",
    "df_eval['sent_count'] = df_eval[\"Tweet\"].apply(lambda x:count_sent(x))\n",
    "df_eval['capital_char_count'] = df_eval[\"Tweet\"].apply(lambda x:count_capital_chars(x))\n",
    "df_eval['capital_word_count'] = df_eval[\"Tweet\"].apply(lambda x:count_capital_words(x))\n",
    "df_eval['quoted_word_count'] = df_eval[\"Tweet\"].apply(lambda x:count_words_in_quotes(x))\n",
    "df_eval['stopword_count'] = df_eval[\"Tweet\"].apply(lambda x:count_stopwords(x))\n",
    "df_eval['unique_word_count'] = df_eval[\"Tweet\"].apply(lambda x:count_unique_words(x))\n",
    "df_eval['htag_count'] = df_eval[\"Tweet\"].apply(lambda x:count_htags(x))\n",
    "df_eval['mention_count'] = df_eval[\"Tweet\"].apply(lambda x:count_mentions(x))\n",
    "df_eval['punct_count'] = df_eval[\"Tweet\"].apply(lambda x:count_punctuations(x))\n",
    "df_eval['avg_wordlength']=df_eval['char_count']/df_eval['word_count']\n",
    "df_eval['avg_sentlength']=df_eval['word_count']/df_eval['sent_count']\n",
    "df_eval['unique_vs_words']=df_eval['unique_word_count']/df_eval['word_count']\n",
    "df_eval['stopwords_vs_words']=df_eval['stopword_count']/df_eval['word_count']\n",
    "df_eval['Tweet'] = df_eval['Tweet'].apply(utils.preprocess_text)\n",
    "df_eval['Tweet'] = df_eval['Tweet'].astype('string')\n",
    "df_eval['Tweet'].fillna('', inplace = True)\n",
    "df_eval_punct = pd.DataFrame(list(df_eval.punct_count))\n",
    "\n",
    "df_eval=pd.merge(df_eval,df_eval_punct,left_index=True, right_index=True)\n",
    "df_eval.drop(columns=['punct_count'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['Tweet'] = df_eval['Tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tf_idf_features  =  vectorizer.transform(df_eval['Tweet'])\n",
    "eval_tf_idf  = pd.DataFrame(eval_tf_idf_features)\n",
    "df_eval = pd.concat([df_eval.reset_index(drop=True), eval_tf_idf.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.drop(columns=['Timestamp'])\n",
    "df_eval['TweetCount'] = df_eval.groupby(group_cols)['Tweet'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_eval \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_eval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36moptimize_dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     17\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[col], downcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m col_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     num_unique \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     num_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[col])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Convert to 'category' if the number of unique values is less than 50% of total entries\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/base.py:1063\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[0;34m(self, dropna)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     uniqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropna:\n\u001b[1;32m   1065\u001b[0m         uniqs \u001b[38;5;241m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7248\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "df_eval = optimize_dataframe(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = ['MatchID', 'PeriodID']\n",
    "agg_dict = {\n",
    "    col : 'count'\n",
    "    for col in df_eval.columns\n",
    "    if col not in group_cols\n",
    "}\n",
    "\n",
    "agg_dict['Tweet'] = lambda x: list(x)\n",
    "agg_dict['TweetCount'] = 'first'\n",
    "\n",
    "agg_df_eval = df_eval.groupby(group_cols).agg(agg_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 516 entries, 0 to 515\n",
      "Data columns (total 52 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   MatchID             516 non-null    uint8 \n",
      " 1   PeriodID            516 non-null    uint8 \n",
      " 2   ID                  516 non-null    int64 \n",
      " 3   Tweet               516 non-null    object\n",
      " 4   char_count          516 non-null    int64 \n",
      " 5   word_count          516 non-null    int64 \n",
      " 6   sent_count          516 non-null    int64 \n",
      " 7   capital_char_count  516 non-null    int64 \n",
      " 8   capital_word_count  516 non-null    int64 \n",
      " 9   quoted_word_count   516 non-null    int64 \n",
      " 10  stopword_count      516 non-null    int64 \n",
      " 11  unique_word_count   516 non-null    int64 \n",
      " 12  htag_count          516 non-null    int64 \n",
      " 13  mention_count       516 non-null    int64 \n",
      " 14  avg_wordlength      516 non-null    int64 \n",
      " 15  avg_sentlength      516 non-null    int64 \n",
      " 16  unique_vs_words     516 non-null    int64 \n",
      " 17  stopwords_vs_words  516 non-null    int64 \n",
      " 18  ! count             516 non-null    int64 \n",
      " 19  \" count             516 non-null    int64 \n",
      " 20  # count             516 non-null    int64 \n",
      " 21  $ count             516 non-null    int64 \n",
      " 22  % count             516 non-null    int64 \n",
      " 23  & count             516 non-null    int64 \n",
      " 24  ' count             516 non-null    int64 \n",
      " 25  ( count             516 non-null    int64 \n",
      " 26  ) count             516 non-null    int64 \n",
      " 27  * count             516 non-null    int64 \n",
      " 28  + count             516 non-null    int64 \n",
      " 29  , count             516 non-null    int64 \n",
      " 30  - count             516 non-null    int64 \n",
      " 31  . count             516 non-null    int64 \n",
      " 32  / count             516 non-null    int64 \n",
      " 33  : count             516 non-null    int64 \n",
      " 34  ; count             516 non-null    int64 \n",
      " 35  < count             516 non-null    int64 \n",
      " 36  = count             516 non-null    int64 \n",
      " 37  > count             516 non-null    int64 \n",
      " 38  ? count             516 non-null    int64 \n",
      " 39  @ count             516 non-null    int64 \n",
      " 40  [ count             516 non-null    int64 \n",
      " 41  \\ count             516 non-null    int64 \n",
      " 42  ] count             516 non-null    int64 \n",
      " 43  ^ count             516 non-null    int64 \n",
      " 44  _ count             516 non-null    int64 \n",
      " 45  ` count             516 non-null    int64 \n",
      " 46  { count             516 non-null    int64 \n",
      " 47  | count             516 non-null    int64 \n",
      " 48  } count             516 non-null    int64 \n",
      " 49  ~ count             516 non-null    int64 \n",
      " 50  0                   516 non-null    int64 \n",
      " 51  TweetCount          516 non-null    int64 \n",
      "dtypes: int64(49), object(1), uint8(2)\n",
      "memory usage: 202.7+ KB\n"
     ]
    }
   ],
   "source": [
    "agg_df_eval.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_eval(grouped_tweets, bert_model, tokenizer, output_file):\n",
    "    \"\"\"\n",
    "    Prépare les features_eval et les labels en utilisant les embeddings BERT, et les sauvegarde dans un fichier.\n",
    "    \n",
    "    :param grouped_tweets: Grouped tweets par MatchID et PeriodID.\n",
    "    :param grouped_labels: Grouped labels par MatchID et PeriodID.\n",
    "    :param bert_model: Modèle BERT pré-entraîné.\n",
    "    :param tokenizer: Tokenizer BERT.\n",
    "    :param output_file: Nom du fichier pour sauvegarder les features_eval et labels.\n",
    "    :return: Tuple (features_eval, labels)\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Chargement des features_eval et labels depuis {output_file}...\")\n",
    "        with open(output_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data[\"grouped_tweets\"], data[\"labels\"]\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    print(\"Calcul des embeddings et labels...\")\n",
    "    for idx, row in tqdm(grouped_tweets.iterrows()):\n",
    "        tweets = row['Tweet']\n",
    "        if not isinstance(tweets, list) or len(tweets) == 0:\n",
    "            embeddings.append(torch.zeros(bert_model.config.hidden_size))  \n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            tweets,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            embed = bert_model.embeddings.word_embeddings(tokenized['input_ids'])\n",
    "        \n",
    "        features_eval_mean = embed.mean(dim=1).mean(dim=0)  # Average across tokens, then across tweets\n",
    "        embeddings.append(features_eval_mean)\n",
    "\n",
    "        match_id, period_id = row['MatchID'], row['PeriodID']\n",
    "\n",
    "    grouped_tweets['Embedding'] = embeddings\n",
    "\n",
    "    print(f\"Sauvegarde des features_eval et labels dans {output_file}...\")\n",
    "    grouped_tweets = grouped_tweets.drop(columns=['Tweet'])\n",
    "    print(f\"Sauvegarde des features_eval et labels dans {output_file}...\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump({\"features_eval\": features_eval_mean}, f)\n",
    "    return grouped_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul des embeddings et labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "516it [12:01,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde des features_eval et labels dans features_eval_labels_eval_spacy.pkl...\n",
      "Sauvegarde des features_eval et labels dans features_eval_labels_eval_spacy.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features_eval = prepare_features_eval(\n",
    "    grouped_tweets=agg_df_eval,\n",
    "    bert_model=bert_model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_file=\"features_eval_labels_eval_spacy.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "embedding_eval= np.vstack(features_eval['Embedding'].apply(convert_embedding).values)\n",
    "\n",
    "\n",
    "columns_to_drop = ['MatchID', 'PeriodID', 'Embedding']\n",
    "\n",
    "X_features_eval = features_eval.drop(columns=columns_to_drop).values\n",
    "X_eval = np.hstack([X_features_eval, embedding_eval])\n",
    "\n",
    "pred_eval = model.predict(X_eval)  \n",
    "\n",
    "submission = pd.read_csv('submission.csv')\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': submission['ID'],  #\n",
    "    'Prediction': pred_eval\n",
    "})\n",
    "\n",
    "# Sauvegarder les prédictions\n",
    "output_df.to_csv(\"submission_ensemble.csv\", index=False)\n",
    "print(\"Predictions saved to submission_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
