{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from difftransformer import DifferentialTransformerClassifier, EmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aminechraibi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aminechraibi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each tweet # 13 min\n",
    "df['Tweet'] = df['Tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet, embeddings_model, vector_size):\n",
    "    \"\"\"\n",
    "    Convert a tweet into a sequence of embeddings.\n",
    "    \"\"\"\n",
    "    tokens = tweet.lower().split()\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embedding = embeddings_model.get(token)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            embeddings.append(np.zeros(vector_size))  # Handle unknown words\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx = 2\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx += 1\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in text.lower().split()]\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_tweet_length, max_tweets_per_period):\n",
    "        self.data = data  # List of periods, each with 'tweets' and 'label'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tweet_length = max_tweet_length\n",
    "        self.max_tweets_per_period = max_tweets_per_period\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        period = self.data.iloc[idx]\n",
    "        tweets = period['Tweet'][:self.max_tweets_per_period]\n",
    "        label = period['EventType']\n",
    "        \n",
    "        # Tokenize and pad tweets\n",
    "        tokenized_tweets = []\n",
    "        for tweet in tweets:\n",
    "            tokens = self.tokenizer(tweet)\n",
    "            tokens = tokens[:self.max_tweet_length]\n",
    "            padding = [0] * (self.max_tweet_length - len(tokens))\n",
    "            tokenized_tweets.append(tokens + padding)\n",
    "        \n",
    "        # Pad the number of tweets if necessary\n",
    "        num_padding_tweets = self.max_tweets_per_period - len(tokenized_tweets)\n",
    "        if num_padding_tweets > 0:\n",
    "            tokenized_tweets.extend([[0] * self.max_tweet_length] * num_padding_tweets)\n",
    "        \n",
    "        tweets_tensor = torch.tensor(tokenized_tweets, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        return tweets_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tweets per period: 57880\n",
      "Maximum number of words in a tweet: 44\n"
     ]
    }
   ],
   "source": [
    "# Group by periodId and count the number of tweets per period\n",
    "tweets_per_period = df.groupby('PeriodID')['Tweet'].count()\n",
    "\n",
    "# Find the maximum number of tweets in any period\n",
    "max_tweets_per_period = tweets_per_period.max()\n",
    "\n",
    "# Calculate the number of words in each tweet\n",
    "df['word_count'] = df['Tweet'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Find the maximum number of words in any tweet\n",
    "max_words_per_tweet = df['word_count'].max()\n",
    "\n",
    "# Display the results\n",
    "print(\"Maximum number of tweets per period:\", max_tweets_per_period)\n",
    "print(\"Maximum number of words in a tweet:\", max_words_per_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for tweets, labels in dataloader:\n",
    "        tweets = tweets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tweets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "all_tweets = df['Tweet'].values\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(all_tweets)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "embedding_dim = 384\n",
    "n_heads = 6\n",
    "depth = 6\n",
    "max_tweet_length = 44\n",
    "max_tweets_per_period = 57880\n",
    "\n",
    "dataset = PeriodDataset(df, tokenizer, max_tweet_length, max_tweets_per_period)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DifferentialTransformerClassifier(vocab_size, embedding_dim, n_heads, depth)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, dataloader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
