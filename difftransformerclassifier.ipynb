{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from difftransformer import DifferentialTransformerClassifier, EmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aminechraibi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aminechraibi/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each tweet # 13 min\n",
    "df['Tweet'] = df['Tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet, embeddings_model, vector_size):\n",
    "    \"\"\"\n",
    "    Convert a tweet into a sequence of embeddings.\n",
    "    \"\"\"\n",
    "    tokens = tweet.lower().split()\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embedding = embeddings_model.get(token)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            embeddings.append(np.zeros(vector_size))  # Handle unknown words\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx = 2\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx += 1\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in text.lower().split()]\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PeriodDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_tweet_length, max_tweets_per_period):\n",
    "        self.data = data  # List of periods, each with 'tweets' and 'label'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tweet_length = max_tweet_length\n",
    "        self.max_tweets_per_period = max_tweets_per_period\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        period = self.data[idx]\n",
    "        tweets = period['tweets'][:self.max_tweets_per_period]\n",
    "        label = period['label']\n",
    "        \n",
    "        # Tokenize and pad tweets\n",
    "        tokenized_tweets = []\n",
    "        for tweet in tweets:\n",
    "            tokens = self.tokenizer(tweet)\n",
    "            tokens = tokens[:self.max_tweet_length]\n",
    "            padding = [0] * (self.max_tweet_length - len(tokens))\n",
    "            tokenized_tweets.append(tokens + padding)\n",
    "        \n",
    "        # Pad the number of tweets if necessary\n",
    "        num_padding_tweets = self.max_tweets_per_period - len(tokenized_tweets)\n",
    "        if num_padding_tweets > 0:\n",
    "            tokenized_tweets.extend([[0] * self.max_tweet_length] * num_padding_tweets)\n",
    "        \n",
    "        tweets_tensor = torch.tensor(tokenized_tweets, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        return tweets_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tweets per period: 28089.166666666668\n",
      "Maximum number of words in a tweet: 44\n"
     ]
    }
   ],
   "source": [
    "# Group by periodId and count the number of tweets per period\n",
    "tweets_per_period = df.groupby('PeriodID')['Tweet'].count()\n",
    "\n",
    "# Find the maximum number of tweets in any period\n",
    "max_tweets_per_period = tweets_per_period.max()\n",
    "\n",
    "# Calculate the number of words in each tweet\n",
    "df['word_count'] = df['Tweet'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Find the maximum number of words in any tweet\n",
    "max_words_per_tweet = df['word_count'].max()\n",
    "\n",
    "# Display the results\n",
    "print(\"Maximum number of tweets per period:\", max_tweets_per_period)\n",
    "print(\"Maximum number of words in a tweet:\", max_words_per_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for tweets, tweet_masks, period_masks, labels in dataloader:\n",
    "        tweets = tweets.to(device)\n",
    "        tweet_masks = tweet_masks.to(device)\n",
    "        period_masks = period_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tweets, tweet_masks, period_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m all_tweets \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizer()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_tweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mvocab_size()\n\u001b[1;32m      7\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m384\u001b[39m\n",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizer.build_vocab\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m      8\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2idx:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2idx[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "all_tweets = df['Tweet'].values\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(all_tweets)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "embedding_dim = 384\n",
    "n_heads = 6\n",
    "depth = 6\n",
    "max_tweet_length = 44\n",
    "max_tweets_per_period = 57880\n",
    "\n",
    "dataset = PeriodDataset(df, tokenizer, max_tweet_length, max_tweets_per_period)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DifferentialTransformerClassifier(vocab_size, embedding_dim, n_heads, depth)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, dataloader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
