{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from difftransformer import DifferentialTransformerClassifier, EmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/amine.chraibi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/amine.chraibi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each tweet # 13 min\n",
    "df['Tweet'] = df['Tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet, embeddings_model, vector_size):\n",
    "    \"\"\"\n",
    "    Convert a tweet into a sequence of embeddings.\n",
    "    \"\"\"\n",
    "    tokens = tweet.lower().split()\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embedding = embeddings_model.get(token)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            embeddings.append(np.zeros(vector_size))  # Handle unknown words\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx = 2\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx += 1\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in text.lower().split()]\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tweets per period: 48710\n",
      "Minimum number of tweets per period: 5062\n",
      "Maximum number of words in a tweet: 44\n"
     ]
    }
   ],
   "source": [
    "# Group by periodId and count the number of tweets per period\n",
    "tweets_per_period = df.groupby('PeriodID')['Tweet'].count()\n",
    "\n",
    "# Find the maximum number of tweets in any period\n",
    "max_tweets_per_period = tweets_per_period.max()\n",
    "min_tweets_per_period = tweets_per_period.min()\n",
    "\n",
    "# Calculate the number of words in each tweet\n",
    "df['word_count'] = df['Tweet'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Find the maximum number of words in any tweet\n",
    "max_words_per_tweet = df['word_count'].max()\n",
    "\n",
    "# Display the results\n",
    "print(\"Maximum number of tweets per period:\", max_tweets_per_period)\n",
    "print(\"Minimum number of tweets per period:\", min_tweets_per_period)\n",
    "print(\"Maximum number of words in a tweet:\", max_words_per_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for tweets, labels in dataloader:\n",
    "        tweets = tweets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tweets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Tokenizer and padding setup\n",
    "all_tweets = df['Tweet'].values\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(all_tweets)\n",
    "\n",
    "MAX_TWEET_LENGTH = 44  # Maximum tweet length in tokens\n",
    "NUM_TWEETS_PER_PERIOD = 92  # Fixed number of tweets per period\n",
    "\n",
    "grouped_tweets = df.groupby(['MatchID', 'PeriodID'])['Tweet'].apply(list).unstack(fill_value=[])\n",
    "grouped_labels = df.groupby(['MatchID', 'PeriodID'])['EventType'].max().unstack(fill_value=0)\n",
    "\n",
    "def pad_tweet(tokens, max_length=MAX_TWEET_LENGTH):\n",
    "    \"\"\"Pad or truncate tokens to max_length.\"\"\"\n",
    "    if len(tokens) < max_length:\n",
    "        return tokens + [tokenizer.word2idx['<PAD>']] * (max_length - len(tokens))\n",
    "    else:\n",
    "        return tokens[:max_length]\n",
    "\n",
    "\n",
    "def sample_tweets_or_pad(period):\n",
    "    \"\"\"Randomly select NUM_TWEETS_PER_PERIOD tweets from the period or pad if empty.\"\"\"\n",
    "    if len(period) == 0:  # If the period is empty\n",
    "        return [[tokenizer.word2idx['<PAD>']] * MAX_TWEET_LENGTH] * NUM_TWEETS_PER_PERIOD\n",
    "\n",
    "    # Randomly sample NUM_TWEETS_PER_PERIOD tweets or pad if fewer\n",
    "    sampled_tweets = random.sample(period, min(len(period), NUM_TWEETS_PER_PERIOD))\n",
    "    padded_tweets = [pad_tweet(tokenizer(tweet)) for tweet in sampled_tweets]\n",
    "\n",
    "    # If fewer than NUM_TWEETS_PER_PERIOD, pad with <PAD> tweets\n",
    "    while len(padded_tweets) < NUM_TWEETS_PER_PERIOD:\n",
    "        padded_tweets.append([tokenizer.word2idx['<PAD>']] * MAX_TWEET_LENGTH)\n",
    "\n",
    "    return padded_tweets\n",
    "\n",
    "\n",
    "def tokenize_and_sample_grouped_tweets(grouped_tweets):\n",
    "    \"\"\"Tokenize tweets and ensure consistent NUM_TWEETS_PER_PERIOD for each period.\"\"\"\n",
    "    tokenized_matches = []\n",
    "    for _, periods in grouped_tweets.iterrows():\n",
    "        tokenized_match = [sample_tweets_or_pad(period) for period in periods]\n",
    "        tokenized_matches.append(tokenized_match)\n",
    "    return tokenized_matches\n",
    "\n",
    "\n",
    "# Tokenize and sample tweets\n",
    "tokenized_and_sampled_tweets = tokenize_and_sample_grouped_tweets(grouped_tweets)\n",
    "labels = grouped_labels.fillna(0).values.tolist()  # Ensure labels are padded with 0 for missing periods\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        match_features = torch.tensor(self.features[idx], dtype=torch.long)  # (num_periods, NUM_TWEETS_PER_PERIOD, MAX_TWEET_LENGTH)\n",
    "        match_labels = torch.tensor(self.labels[idx], dtype=torch.bfloat16)  # (num_periods,)\n",
    "        return match_features, match_labels\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TweetDataset(tokenized_and_sampled_tweets, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "depth = 2\n",
    "batch_size = 1\n",
    "n_embd = 162\n",
    "n_head = 3\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.key_1 = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query_1 = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.key_2 = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query_2 = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x, lamb):\n",
    "        B, T, C = x.shape\n",
    "        k_1 = self.key_1(x)\n",
    "        q_1 = self.query_1(x)\n",
    "        k_2 = self.key_2(x)\n",
    "        q_2 = self.query_2(x)\n",
    "        wei_1 = q_1 @ k_1.transpose(-2,-1) * k_1.shape[-1]**-0.5\n",
    "        wei_2 = q_2 @ k_2.transpose(-2,-1) * k_2.shape[-1]**-0.5\n",
    "        wei_1 = F.softmax(wei_1, dim=-1)\n",
    "        wei_2 = F.softmax(wei_2, dim=-1)\n",
    "        v = self.value(x)\n",
    "        wei = wei_1 - lamb * wei_2\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadDifferentialAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, lambda_init=0.8):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embedding_dim, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.lamb = nn.Parameter(torch.tensor(lambda_init))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x, self.lamb) for h in self.heads], dim=-1)\n",
    "        out = self.norm(out)\n",
    "        out = (1 - self.lamb) * out\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadDifferentialAttention(embedding_dim, num_heads)\n",
    "        self.ffwd = FeedFoward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        # embedding_dim: embedding dimension, num_heads: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "        self.sa = MultiHeadDifferentialAttention(embedding_dim, num_heads)\n",
    "        self.ffwd = FeedFoward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TweetEncoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of tokens into a single tweet embedding.\"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(TweetEncoder, self).__init__()\n",
    "        self.attention = MultiHeadDifferentialAttention(embedding_dim, num_heads)\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        tweet_embedding = attn_output.mean(dim=1)\n",
    "        return self.fc(tweet_embedding)\n",
    "\n",
    "# Define the period encoder\n",
    "class PeriodEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, depth):\n",
    "        super(PeriodEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([Block(embedding_dim, num_heads) for _ in range(depth)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the classification head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "    \n",
    "\n",
    "# Define the full model\n",
    "class DifferentialTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, depth):\n",
    "        super(DifferentialTransformerClassifier, self).__init__()\n",
    "        self.embedding = EmbeddingLayer(vocab_size, embedding_dim)\n",
    "        self.tweet_encoder = TweetEncoder(embedding_dim, num_heads)\n",
    "        self.period_encoder = PeriodEncoder(embedding_dim, num_heads, depth)\n",
    "        self.classifier = ClassificationHead(embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Tensor of shape (batch_size, num_periods, num_tweets_per_period, tweet_length)\n",
    "                        Represents the tokenized and padded tweets grouped by period.\n",
    "        Returns:\n",
    "            Output: Tensor of shape (batch_size,)\n",
    "                    Binary predictions for each period in the batch.\n",
    "        \"\"\"\n",
    "        batch_size, num_periods, num_tweets_per_period, tweet_length = features.shape\n",
    "\n",
    "        # Flatten for embedding\n",
    "        tweets_flat = features.view(-1, tweet_length)  # (batch_size * num_periods * num_tweets_per_period, tweet_length)\n",
    "        x = self.embedding(tweets_flat)  # (batch_size * num_periods * num_tweets_per_period, tweet_length, embedding_dim)\n",
    "        # Encode each tweet\n",
    "        x = self.tweet_encoder(x)  # (batch_size * num_periods * num_tweets_per_period, embedding_dim)\n",
    "\n",
    "        # Reshape back to periods\n",
    "        x = x.view(batch_size * num_periods, num_tweets_per_period, self.embedding_dim)  # (batch_size * num_periods, num_tweets_per_period, embedding_dim)\n",
    "        \n",
    "        # Encode the periods\n",
    "        x = self.period_encoder(x)  # (batch_size * num_periods, num_tweets_per_period, embedding_dim)\n",
    "\n",
    "        # Pool over periods\n",
    "        x = x.mean(dim=1)  # (batch_size * num_periods, embedding_dim)\n",
    "                \n",
    "        # Classification\n",
    "        out = self.classifier(x)  # (batch_size, 1)\n",
    "        \n",
    "        out = out.view(batch_size, num_periods)  # (batch_size, num_periods)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DifferentialTransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=n_embd,  # Ensure this matches the dimension used in embeddings\n",
    "    num_heads=n_head,\n",
    "    depth=depth\n",
    ").to(device)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.6310\n",
      "Epoch 3/10, Loss: 0.5617\n",
      "Epoch 4/10, Loss: 0.5312\n",
      "Epoch 5/10, Loss: 0.5219\n",
      "Epoch 6/10, Loss: 0.5147\n",
      "Epoch 7/10, Loss: 0.5095\n",
      "Epoch 8/10, Loss: 0.5095\n",
      "Epoch 9/10, Loss: 0.5033\n",
      "Epoch 10/10, Loss: 0.5024\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, dataloader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataloader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tweets, labels in dataloader:\n",
    "            tweets = tweets.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(tweets)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().float().numpy())\n",
    "            all_predictions.extend((outputs.cpu().float().numpy() > 0.5).astype(int))\n",
    "    \n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DifferentialTransformerClassifier(vocab_size, embedding_dim, n_heads, depth)\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "# model.to(device)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "eval_li = []\n",
    "for filename in os.listdir(\"val_tweets\"):\n",
    "    df_eval = pd.read_csv(\"val_tweets/\" + filename)\n",
    "    eval_li.append(df_eval)\n",
    "df_eval = pd.concat(eval_li, ignore_index=True)\n",
    "\n",
    "# Preprocess the evaluation data\n",
    "df_eval['Tweet'] = df_eval['Tweet'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18_0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1276869000000</td>\n",
       "      <td>usa stateside follower stand represent beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18_0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1276869000000</td>\n",
       "      <td>lynz_ think ref might basil fawlty actually wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18_0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1276869000000</td>\n",
       "      <td>hoping usa win help ease pain last night loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18_0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1276869000000</td>\n",
       "      <td>actually start worldcup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18_0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1276869000000</td>\n",
       "      <td>hanson roy proper pundit line worldcup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "0  18_0       18         0          0  1276869000000   \n",
       "1  18_0       18         0          0  1276869000000   \n",
       "2  18_0       18         0          0  1276869000000   \n",
       "3  18_0       18         0          0  1276869000000   \n",
       "4  18_0       18         0          0  1276869000000   \n",
       "\n",
       "                                               Tweet  \n",
       "0  usa stateside follower stand represent beautif...  \n",
       "1  lynz_ think ref might basil fawlty actually wo...  \n",
       "2  hoping usa win help ease pain last night loss ...  \n",
       "3                            actually start worldcup  \n",
       "4             hanson roy proper pundit line worldcup  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PeriodDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Prepare the evaluation dataset and dataloader\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPeriodDataset\u001b[49m(df_eval, tokenizer, max_tweet_length, max_tweets_per_period)\n\u001b[1;32m     11\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(eval_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PeriodDataset' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_tweets = df_eval.groupby(['MatchID', 'PeriodID'])['Tweet'].apply(list).unstack(fill_value=[])\n",
    "grouped_labels = df_eval.groupby(['MatchID', 'PeriodID'])['EventType'].max().unstack(fill_value=0)\n",
    "tokenized_and_sampled_tweets = tokenize_and_sample_grouped_tweets(grouped_tweets)\n",
    "labels = grouped_labels.fillna(0).values.tolist() \n",
    "# Create Dataset and DataLoader\n",
    "dataset = TweetDataset(tokenized_and_sampled_tweets, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.7396\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.33      1.00      0.50         1\n",
      "           3       0.67      1.00      0.80         2\n",
      "           4       0.33      1.00      0.50         1\n",
      "           5       0.67      1.00      0.80         2\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       0.67      1.00      0.80         2\n",
      "          10       0.67      1.00      0.80         2\n",
      "          11       0.67      1.00      0.80         2\n",
      "          12       0.67      1.00      0.80         2\n",
      "          13       0.33      1.00      0.50         1\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.33      1.00      0.50         1\n",
      "          23       0.00      0.00      0.00         0\n",
      "          24       0.33      1.00      0.50         1\n",
      "          25       0.33      1.00      0.50         1\n",
      "          26       0.33      1.00      0.50         1\n",
      "          27       0.67      1.00      0.80         2\n",
      "          28       0.67      1.00      0.80         2\n",
      "          29       0.33      1.00      0.50         1\n",
      "          30       0.33      1.00      0.50         1\n",
      "          31       0.33      1.00      0.50         1\n",
      "          32       0.33      1.00      0.50         1\n",
      "          33       0.33      1.00      0.50         1\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.33      1.00      0.50         1\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.33      1.00      0.50         1\n",
      "          38       0.33      1.00      0.50         1\n",
      "          39       0.33      1.00      0.50         1\n",
      "          40       0.33      1.00      0.50         1\n",
      "          41       0.33      1.00      0.50         1\n",
      "          42       0.33      1.00      0.50         1\n",
      "          43       0.33      1.00      0.50         1\n",
      "          44       0.33      1.00      0.50         1\n",
      "          45       1.00      1.00      1.00         3\n",
      "          46       1.00      1.00      1.00         3\n",
      "          47       1.00      1.00      1.00         3\n",
      "          48       0.67      1.00      0.80         2\n",
      "          49       0.67      1.00      0.80         2\n",
      "          50       0.67      1.00      0.80         2\n",
      "          51       0.67      1.00      0.80         2\n",
      "          52       0.33      1.00      0.50         1\n",
      "          53       0.33      1.00      0.50         1\n",
      "          54       0.67      1.00      0.80         2\n",
      "          55       0.33      1.00      0.50         1\n",
      "          56       1.00      1.00      1.00         3\n",
      "          57       1.00      1.00      1.00         3\n",
      "          58       1.00      1.00      1.00         3\n",
      "          59       1.00      1.00      1.00         3\n",
      "          60       1.00      1.00      1.00         3\n",
      "          61       1.00      1.00      1.00         3\n",
      "          62       1.00      1.00      1.00         3\n",
      "          63       0.67      1.00      0.80         2\n",
      "          64       0.67      1.00      0.80         2\n",
      "          65       0.00      0.00      0.00         0\n",
      "          66       0.00      0.00      0.00         0\n",
      "          67       0.00      0.00      0.00         0\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         0\n",
      "          70       0.00      0.00      0.00         0\n",
      "          71       0.33      1.00      0.50         1\n",
      "          72       0.33      1.00      0.50         1\n",
      "          73       1.00      1.00      1.00         3\n",
      "          74       0.67      1.00      0.80         2\n",
      "          75       1.00      1.00      1.00         3\n",
      "          76       0.33      1.00      0.50         1\n",
      "          77       1.00      1.00      1.00         3\n",
      "          78       0.67      1.00      0.80         2\n",
      "          79       0.33      1.00      0.50         1\n",
      "          80       0.33      1.00      0.50         1\n",
      "          81       0.33      1.00      0.50         1\n",
      "          82       0.33      1.00      0.50         1\n",
      "          83       0.67      1.00      0.80         2\n",
      "          84       1.00      1.00      1.00         3\n",
      "          85       0.67      1.00      0.80         2\n",
      "          86       0.33      1.00      0.50         1\n",
      "          87       0.67      1.00      0.80         2\n",
      "          88       0.33      1.00      0.50         1\n",
      "          89       0.33      1.00      0.50         1\n",
      "          90       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         0\n",
      "          92       0.33      1.00      0.50         1\n",
      "          93       0.33      1.00      0.50         1\n",
      "          94       0.00      0.00      0.00         0\n",
      "          95       0.33      1.00      0.50         1\n",
      "          96       0.67      1.00      0.80         2\n",
      "          97       0.67      1.00      0.80         2\n",
      "          98       0.67      1.00      0.80         2\n",
      "          99       0.67      1.00      0.80         2\n",
      "         100       0.00      0.00      0.00         0\n",
      "         101       0.00      0.00      0.00         0\n",
      "         102       0.67      1.00      0.80         2\n",
      "         103       0.67      1.00      0.80         2\n",
      "         104       0.33      1.00      0.50         1\n",
      "         105       0.33      1.00      0.50         1\n",
      "         106       0.67      1.00      0.80         2\n",
      "         107       0.67      1.00      0.80         2\n",
      "         108       0.67      1.00      0.80         2\n",
      "         109       1.00      1.00      1.00         3\n",
      "         110       1.00      1.00      1.00         3\n",
      "         111       1.00      1.00      1.00         3\n",
      "         112       0.67      1.00      0.80         2\n",
      "         113       1.00      1.00      1.00         3\n",
      "         114       1.00      1.00      1.00         3\n",
      "         115       0.67      1.00      0.80         2\n",
      "         116       0.33      1.00      0.50         1\n",
      "         117       0.33      1.00      0.50         1\n",
      "         118       1.00      1.00      1.00         3\n",
      "         119       0.67      1.00      0.80         2\n",
      "         120       0.67      1.00      0.80         2\n",
      "         121       0.67      1.00      0.80         2\n",
      "         122       0.67      1.00      0.80         2\n",
      "         123       1.00      1.00      1.00         3\n",
      "         124       1.00      1.00      1.00         3\n",
      "         125       1.00      1.00      1.00         3\n",
      "         126       1.00      1.00      1.00         3\n",
      "         127       1.00      1.00      1.00         3\n",
      "         128       1.00      1.00      1.00         3\n",
      "         129       0.67      1.00      0.80         2\n",
      "\n",
      "   micro avg       0.51      1.00      0.68       199\n",
      "   macro avg       0.51      0.82      0.60       199\n",
      "weighted avg       0.74      1.00      0.82       199\n",
      " samples avg       0.51      1.00      0.67       199\n",
      "\n",
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/amine.chraibi/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "eval_loss, eval_labels, eval_predictions = evaluate(model, dataloader, criterion, device)\n",
    "\n",
    "# Print evaluation results\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(eval_labels, eval_predictions))\n",
    "print(f\"Accuracy: {accuracy_score(eval_labels, eval_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
