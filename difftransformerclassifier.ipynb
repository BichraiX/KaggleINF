{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/amine.chraibi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/amine.chraibi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset\n",
      "Data and tokenizer ready\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from difftransformer import DifferentialTransformerClassifier, EmbeddingLayer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = utils.tokenizer.vocab_size()\n",
    "depth = 5\n",
    "n_embd = 144\n",
    "n_head = 4\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dropout = 0.014500254910782884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each evaluation file\n",
    "li = []\n",
    "for filename in os.listdir(\"eval_tweets\"):\n",
    "    test_df = pd.read_csv(\"eval_tweets/\" + filename)\n",
    "    li.append(test_df)\n",
    "test_df = pd.concat(li, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Tweet'] = test_df['Tweet'].apply(utils.preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/amine.chraibi/KaggleINF/difftransformer.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.lamb = nn.Parameter(torch.tensor(lambda_init))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model\n",
    "model = DifferentialTransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=n_embd,  # Ensure this matches the dimension used in embeddings\n",
    "    num_heads=n_head,\n",
    "    depth=depth,\n",
    "    dropout = dropout\n",
    ")\n",
    "model.load_state_dict(torch.load(\"model_checkpoint_10.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare for predictions\n",
    "def majority_vote(subperiod_predictions):\n",
    "    \"\"\"\n",
    "    Perform majority vote on subperiod predictions to determine the overall period prediction.\n",
    "    \"\"\"\n",
    "    return max(set(subperiod_predictions), key=subperiod_predictions.count)\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "test_dataset, period_to_subperiod_mapping = utils.prepare_dataset(test_df, False)\n",
    "test_dataset = torch.tensor(test_dataset, device = device)\n",
    "# Predict on subperiods\n",
    "subperiod_predictions = []\n",
    "with torch.no_grad():\n",
    "    for tweets in test_dataset:\n",
    "        tweets = tweets.unsqueeze(0) # add batch dimension\n",
    "        outputs = model(tweets)  # (batch_size, )\n",
    "        preds = (outputs.float().cpu().numpy() > 0.5).astype(int).tolist()  # Binary predictions\n",
    "        subperiod_predictions.extend(preds)\n",
    "\n",
    "# Aggregate subperiod predictions into period predictions\n",
    "period_predictions = {}\n",
    "for (match_id,period_id), subperiod_indices in period_to_subperiod_mapping.items():\n",
    "    if not subperiod_indices :\n",
    "        continue\n",
    "    subperiod_preds = [subperiod_predictions[idx] for idx in subperiod_indices]\n",
    "    period_predictions[f'{match_id}_{period_id}'] = majority_vote(subperiod_preds)\n",
    "\n",
    "# Save predictions\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': list(period_predictions.keys()),\n",
    "    'Prediction': list(period_predictions.values())\n",
    "})\n",
    "\n",
    "output_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to submission.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
